# ═══════════════════════════════════════════════════════════════════════════════
# OLLAMA DUAL-GPU OPTIMIZATION - Engineering Grade Configuration
# ═══════════════════════════════════════════════════════════════════════════════
# 
# Hardware:
#   GPU[0]: AMD Radeon Pro W5700X (gfx1010) - 16GB VRAM, 40 CUs
#   GPU[1]: AMD Radeon RX 6900 XT (gfx1030) - 16GB VRAM, 80 CUs
#   Total: 32GB VRAM, 120 Compute Units
#
# Sources:
#   - https://docs.ollama.com/faq (concurrent requests, multi-GPU, flash attention)
#   - https://github.com/ollama/ollama/blob/main/envconfig/config.go
#   - https://rocm.docs.amd.com/en/latest/conceptual/gpu-isolation.html
#
# Install: sudo cp ~/.roxy/ollama-dual-gpu.conf /etc/systemd/system/ollama.service.d/
#          sudo systemctl daemon-reload && sudo systemctl restart ollama
# ═══════════════════════════════════════════════════════════════════════════════

[Service]
# ─────────────────────────────────────────────────────────────────────────────
# GPU VISIBILITY - CRITICAL FIX
# Source: AMD ROCm GPU Isolation docs
# ─────────────────────────────────────────────────────────────────────────────
# PROBLEM: ROCR_VISIBLE_DEVICES=1 was hiding GPU[0] (W5700X)!
# FIX: Make BOTH GPUs visible
Environment="ROCR_VISIBLE_DEVICES=0,1"
Environment="HIP_VISIBLE_DEVICES=0,1"

# ─────────────────────────────────────────────────────────────────────────────
# MULTI-GPU SCHEDULING
# Source: Ollama config.go - OLLAMA_SCHED_SPREAD
# ─────────────────────────────────────────────────────────────────────────────
# "Always schedule model across all GPUs" - spreads load instead of picking one
Environment="OLLAMA_SCHED_SPREAD=1"

# ─────────────────────────────────────────────────────────────────────────────
# PARALLEL PROCESSING
# Source: https://docs.ollama.com/faq - "How does Ollama handle concurrent requests?"
# ─────────────────────────────────────────────────────────────────────────────
# Default is 1 or 4 based on memory. With 32GB, we can do 8.
# "Parallel request processing for a given model results in increasing the 
#  context size by the number of parallel requests"
Environment="OLLAMA_NUM_PARALLEL=8"

# Maximum loaded models (default: 3 * num_gpus = 6 for us)
Environment="OLLAMA_MAX_LOADED_MODELS=4"

# ─────────────────────────────────────────────────────────────────────────────
# FLASH ATTENTION
# Source: https://docs.ollama.com/faq - "How can I enable Flash Attention?"
# ─────────────────────────────────────────────────────────────────────────────
# "Flash Attention can significantly reduce memory usage as context grows"
Environment="OLLAMA_FLASH_ATTENTION=1"

# ─────────────────────────────────────────────────────────────────────────────
# KV CACHE QUANTIZATION
# Source: https://docs.ollama.com/faq - "How can I set the quantization type?"
# ─────────────────────────────────────────────────────────────────────────────
# q8_0 = half memory of f16, "usually has no noticeable impact on quality"
Environment="OLLAMA_KV_CACHE_TYPE=q8_0"

# ─────────────────────────────────────────────────────────────────────────────
# CONTEXT & MEMORY
# ─────────────────────────────────────────────────────────────────────────────
Environment="OLLAMA_CONTEXT_LENGTH=8192"
Environment="OLLAMA_KEEP_ALIVE=30m"

# ─────────────────────────────────────────────────────────────────────────────
# AMD-SPECIFIC COMPATIBILITY
# ─────────────────────────────────────────────────────────────────────────────
# W5700X is gfx1010, this helps with ROCm compatibility
Environment="HSA_OVERRIDE_GFX_VERSION=10.3.0"

# Network binding (already set, keeping it)
Environment="OLLAMA_HOST=0.0.0.0"

# ─────────────────────────────────────────────────────────────────────────────
# QUEUE MANAGEMENT
# Source: Ollama docs - default is 512, fine for our use
# ─────────────────────────────────────────────────────────────────────────────
Environment="OLLAMA_MAX_QUEUE=512"
