# üñ•Ô∏è Infrastructure Audit & GPU Optimization Analysis

**Date**: January 1, 2026  
**System**: Mac Pro 2019  
**Issue**: RX 6900 XT hitting 100% GPU usage during roxy testing

---

## üìä Current Infrastructure

### GPU Hardware
- **Primary GPU**: AMD Radeon RX 6900 XT (Navi 21)
  - **VRAM**: 16GB GDDR6
  - **Compute Units**: 80 CUs
  - **Peak Performance**: ~23 TFLOPS (FP32)
  - **Status**: ‚úÖ Excellent for AI workloads

- **Secondary GPU**: AMD Radeon Pro W5700X (Navi 10)
  - **VRAM**: 16GB GDDR6
  - **Status**: Available for offloading or display

### CPU Hardware
- **Processor**: Intel Xeon W-3275
  - **Cores**: 28 physical cores
  - **Threads**: 56 (with hyperthreading)
  - **Base Clock**: 2.5 GHz
  - **Max Turbo**: 4.6 GHz
  - **Status**: ‚úÖ Extremely powerful, underutilized

### Memory
- **Total RAM**: 157GB
- **Available**: 122GB
- **Status**: ‚úÖ Excellent headroom

---

## üîç Current GPU Usage Analysis

### Models Currently Using GPU

| Component | Model | VRAM Usage | GPU Load | Notes |
|-----------|-------|------------|----------|-------|
| **Whisper** | large-v3 | ~3-4 GB | 60-80% | Transcription (faster-whisper) |
| **Ollama LLM** | llama3:8b | ~8-10 GB | 40-70% | Main LLM inference |
| **TTS** | XTTS v2 | ~2-3 GB | 30-50% | Text-to-speech |
| **Total Peak** | - | **~15-17 GB** | **100%** | ‚ö†Ô∏è **Exceeds 16GB VRAM!** |

### The Problem

When testing roxy, all three models load simultaneously:
1. **Whisper large-v3** loads for transcription
2. **Ollama llama3:8b** loads for LLM responses
3. **XTTS v2** loads for voice output

**Result**: 15-17GB VRAM needed, but only 16GB available ‚Üí GPU hits 100% and may cause:
- Out-of-memory errors
- Model swapping (slow)
- Performance degradation
- System instability

---

## üí° Optimization Strategy

### Option 1: Hybrid CPU/GPU Distribution (RECOMMENDED)

**Best approach**: Distribute workloads intelligently between CPU and GPU.

#### Strategy:
1. **Keep on GPU** (high priority, real-time):
   - Ollama LLM (llama3:8b) - **8-10GB VRAM**
   - TTS (XTTS v2) - **2-3GB VRAM**
   - **Total GPU**: ~10-13GB VRAM ‚úÖ

2. **Move to CPU** (can tolerate slight delay):
   - Whisper transcription (large-v3) - **Use CPU with 56 threads**
   - CPU performance: ~1-2x real-time (acceptable for most use cases)
   - **Frees**: ~3-4GB VRAM

3. **Benefits**:
   - ‚úÖ GPU stays under 80% utilization
   - ‚úÖ No VRAM overflow
   - ‚úÖ Better overall system balance
   - ‚úÖ CPU is powerful enough for Whisper

#### Implementation:
```python
# Whisper: Force CPU usage
WHISPER_DEVICE = "cpu"  # Use powerful 56-thread CPU
WHISPER_COMPUTE_TYPE = "float32"

# Ollama & TTS: Keep on GPU
OLLAMA_GPU = true
TTS_GPU = true
```

### Option 2: Use Smaller Models

#### Whisper Model Sizes:
- **large-v3**: 3-4GB VRAM, best accuracy
- **medium**: 1.5-2GB VRAM, good accuracy
- **base**: 0.5-1GB VRAM, acceptable accuracy
- **small**: 0.3-0.5GB VRAM, decent accuracy

**Recommendation**: Use **medium** or **base** for real-time transcription
- Saves 2-3GB VRAM
- Still excellent quality
- Faster inference

#### Ollama Model Options:
- **llama3:8b**: 8-10GB VRAM (current)
- **llama3.2:1b**: 1-2GB VRAM (much smaller, less capable)
- **llama3.2:latest**: 2-3GB VRAM (good balance)

**Recommendation**: Keep **llama3:8b** (best quality), but ensure it's the only large model on GPU

### Option 3: Smart Model Loading

Implement dynamic model loading:
- Load models on-demand
- Unload when idle
- Share GPU memory efficiently

**Example**:
```python
# Only load Whisper when needed
if transcription_needed:
    load_whisper_model()
    transcribe()
    unload_whisper_model()  # Free VRAM
```

### Option 4: Use Secondary GPU (W5700X)

If both GPUs are available:
- **RX 6900 XT**: Ollama + TTS
- **W5700X**: Whisper transcription

**Requires**: Multi-GPU setup in PyTorch/ROCm

---

## üéØ Recommended Solution

### **Hybrid Approach: CPU for Whisper, GPU for LLM/TTS**

**Why this works best**:
1. ‚úÖ Your CPU is extremely powerful (56 threads)
2. ‚úÖ Whisper on CPU is still fast enough (1-2x real-time)
3. ‚úÖ LLM and TTS need GPU for real-time performance
4. ‚úÖ Keeps GPU under 80% utilization
5. ‚úÖ No VRAM overflow
6. ‚úÖ Better system balance

### Performance Impact:

| Component | Current (GPU) | Optimized (CPU) | Impact |
|-----------|---------------|-----------------|--------|
| Whisper | 5-10x real-time | 1-2x real-time | ‚ö†Ô∏è Slower but acceptable |
| Ollama | 20-50 tok/s | 20-50 tok/s | ‚úÖ No change |
| TTS | 0.2-0.5s/sentence | 0.2-0.5s/sentence | ‚úÖ No change |

**Net Result**: Slight delay in transcription (acceptable), but stable GPU usage.

---

## üöÄ Do You Need a Better GPU?

### Current GPU: RX 6900 XT (16GB)
- **Verdict**: ‚úÖ **Good enough** with optimization
- **Issue**: Not GPU power, but **VRAM capacity**
- **Solution**: Better resource management, not hardware upgrade

### If You Still Want More GPU Power:

#### Option A: AMD Radeon RX 7900 XTX (24GB)
- **VRAM**: 24GB (50% more)
- **Cost**: ~$900-1000
- **Benefit**: Can run all models simultaneously
- **Verdict**: ‚ö†Ô∏è Overkill if optimized properly

#### Option B: NVIDIA RTX 4090 (24GB)
- **VRAM**: 24GB
- **Performance**: Better AI performance than AMD
- **Cost**: ~$1600-2000
- **Issue**: Requires switching from ROCm to CUDA
- **Verdict**: ‚ùå Not worth the migration cost

#### Option C: Keep RX 6900 XT + Optimize
- **Cost**: $0
- **Benefit**: Proper resource management
- **Verdict**: ‚úÖ **RECOMMENDED**

---

## üìã Implementation Plan

### Phase 1: Immediate Fixes (CPU Offloading)

1. **Configure Whisper to use CPU**:
   ```bash
   # Update voice/transcription/service.py
   device = "cpu"  # Force CPU for Whisper
   compute_type = "float32"
   ```

2. **Keep Ollama and TTS on GPU**:
   ```bash
   # Ensure these stay on GPU
   OLLAMA_GPU=true
   TTS_GPU=true
   ```

3. **Test GPU usage**:
   ```bash
   # Monitor GPU usage
   rocm-smi
   # Should see ~60-70% max instead of 100%
   ```

### Phase 2: Model Size Optimization

1. **Consider smaller Whisper model**:
   - Test `medium` or `base` instead of `large-v3`
   - Evaluate quality vs. performance trade-off

2. **Keep Ollama llama3:8b** (best quality)

### Phase 3: Smart Loading (Advanced)

1. Implement dynamic model loading
2. Unload models when idle
3. Monitor VRAM usage

---

## üîß Configuration Changes Needed

### 1. Whisper Configuration
```python
# voice/transcription/service.py
device = "cpu"  # Change from "auto" or "cuda"
compute_type = "float32"  # CPU uses float32
```

### 2. Environment Variables
```bash
# .env file
ROXY_WHISPER_DEVICE=cpu
ROXY_WHISPER_COMPUTE_TYPE=float32
ROXY_GPU_ENABLED=true  # For Ollama and TTS
```

### 3. Content Pipeline
```python
# content-pipeline/transcriber.py
MODEL_SIZE = 'medium'  # Or 'base' instead of 'large-v3'
DEVICE = 'cpu'
COMPUTE_TYPE = 'float32'
```

---

## üìä Expected Results After Optimization

### GPU Usage:
- **Before**: 100% GPU, 15-17GB VRAM (overflow)
- **After**: 60-70% GPU, 10-13GB VRAM ‚úÖ

### Performance:
- **Whisper**: 1-2x real-time (CPU) vs 5-10x (GPU)
  - Still fast enough for real-time use
- **LLM**: No change (still on GPU)
- **TTS**: No change (still on GPU)

### System Stability:
- ‚úÖ No VRAM overflow
- ‚úÖ No GPU throttling
- ‚úÖ Better thermal management
- ‚úÖ More headroom for other tasks

---

## üéì Key Insights

1. **Your CPU is powerful enough** for Whisper transcription
   - 56 threads can handle it efficiently
   - 1-2x real-time is acceptable

2. **GPU bottleneck is VRAM, not compute**
   - RX 6900 XT has plenty of compute power
   - 16GB VRAM is the limiting factor

3. **Hybrid approach is optimal**
   - Use GPU for what needs it (LLM, TTS)
   - Use CPU for what can tolerate it (transcription)

4. **No hardware upgrade needed**
   - Optimization is better than buying new GPU
   - Save $1000+ and get better results

---

## ‚úÖ Action Items

1. ‚úÖ **Immediate**: Configure Whisper to use CPU
2. ‚úÖ **Test**: Verify GPU usage drops to 60-70%
3. ‚úÖ **Monitor**: Check transcription quality on CPU
4. ‚ö†Ô∏è **Optional**: Consider smaller Whisper model if needed
5. ‚ö†Ô∏è **Future**: Implement smart model loading

---

## üìû Next Steps

1. Review this analysis
2. Approve CPU offloading for Whisper
3. Implement configuration changes
4. Test and verify GPU usage
5. Monitor performance and adjust if needed

---

**Conclusion**: Your RX 6900 XT is sufficient. The issue is resource management, not hardware capability. Moving Whisper to CPU will solve the 100% GPU usage problem while maintaining excellent performance.







